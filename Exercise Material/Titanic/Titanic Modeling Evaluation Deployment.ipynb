{"cells": [{"metadata": {}, "cell_type": "markdown", "source": "Reading and writing files is slightly different between Local and Cloud. In the Cloud, the easiest way is to use project-lib (see https://dataplatform.cloud.ibm.com/docs/content/wsj/analyze-data/project-lib-python.html?audience=wdp if you want to learn more). When opening this notebook in Watson Studio Cloud for the first time, insert a project token by clicking on the \"hamburger\" icon on the right hand side. If no project token exists, follow the link to create a new one. Then, return to this notebook and repeat the steps to insert a project token. This will now add an additional cell above. Run this cell!"}, {"metadata": {}, "cell_type": "markdown", "source": "# Titanic Modeling, Evaluation and Deployment"}, {"metadata": {}, "cell_type": "markdown", "source": "## CRISP-DM"}, {"metadata": {}, "cell_type": "code", "source": "from IPython import display\ndisplay.Image('https://www.kdnuggets.com/wp-content/uploads/crisp-dm-4-problems-fig1.png', width=500)\n", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "## Import relevant packages"}, {"metadata": {}, "cell_type": "code", "source": "import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set()\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn import metrics\nfrom sklearn.metrics import confusion_matrix, classification_report", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "## Load data and prepare for modeling"}, {"metadata": {}, "cell_type": "markdown", "source": "Data preparation for modeling (including 'pd.get_dummies') has been performed in the previous notebook. Now, data are imported again and split into training and test. Models are built on training data only and, afterwards, evaluated on (previously unseen) test data. "}, {"metadata": {}, "cell_type": "markdown", "source": "Attention: As stated previously, loading data differs between local and cloud versions, select the right one depending on the platform used"}, {"metadata": {}, "cell_type": "code", "source": "# Local\n# df_dummies = pd.read_csv('train_dummies.csv') # use full path if notebook and file in different folders! \n\n# Cloud: Fetch the file\nmy_file = project.get_file(\"train_dummies.csv\")\n\n# Cloud: Read the CSV data file from the object storage into a pandas DataFrame\nmy_file.seek(0)\nimport pandas as pd\ndf_dummies = pd.read_csv(my_file)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "df_dummies.head()", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "target = df_dummies['Survived'] # feature to be predicted\npredictors = df_dummies.drop(['Survived'], axis = 1) # all other features are used as predictors", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "predictors.head()", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "X_train, X_test, y_train, y_test = train_test_split(predictors, target, test_size=0.2, random_state=123) # 80-20 split into training and test data", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "# Check data balancing\ny_train.value_counts()\n\n# There is no severe skew in the class distribution. No resampling needed. \n# If you want to learn more about resampling, also check https://machinelearningmastery.com/random-oversampling-and-undersampling-for-imbalanced-classification/", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "## Create and evaluate classification models"}, {"metadata": {}, "cell_type": "markdown", "source": "Predicting whether a pasenger on the Titanic survived or not is a supervised machine learning problem. Some commonly used algorithms include decision trees, random forest and logistic regression. Once a classification model has been built, evaluation metrics are calculated and interpreted. "}, {"metadata": {}, "cell_type": "markdown", "source": "### Decision Tree"}, {"metadata": {}, "cell_type": "code", "source": "tree = DecisionTreeClassifier()\ntree.fit(X_train, y_train)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "confusion_matrix(y_test, tree.predict(X_test)) # yields count of true negatives, false positives, false negatives, true positives", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "tn, fp, fn, tp = confusion_matrix(y_test, tree.predict(X_test)).ravel() # check that tp, fp, tn, fn are not confused\nprint(tn, fp, fn, tp)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "print(classification_report(y_train, tree.predict(X_train))) # yields class-specific precision, recall and f1-score", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "print(classification_report(y_test, tree.predict(X_test)))", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "Performance on test data is significantly lower than on training data. Probably the decision tree overfits on training data and does not generalize well on unseen test data. "}, {"metadata": {}, "cell_type": "code", "source": "list(zip(X_train.columns, tree.feature_importances_)) # lists features and their importance in predicting the target", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "### Random Forest"}, {"metadata": {}, "cell_type": "code", "source": "rf = RandomForestClassifier()\nrf.fit(X_train, y_train)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "confusion_matrix(y_test, rf.predict(X_test))", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "print(classification_report(y_train, rf.predict(X_train)))", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "print(classification_report(y_test, rf.predict(X_test)))", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "As before, test performance is lower than training performance. Random forests, too, can suffer from overfitting on training data. "}, {"metadata": {}, "cell_type": "code", "source": "list(zip(X_train.columns, rf.feature_importances_))", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "### Logistic Regression"}, {"metadata": {}, "cell_type": "code", "source": "logreg = LogisticRegression()\nlogreg.fit(X_train, y_train)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "print(confusion_matrix(y_test, logreg.predict(X_test)))", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "# nicer way to inspect confusion matrix\nconf_mat = confusion_matrix(y_test, logreg.predict(X_test))\ndf_cm = pd.DataFrame(conf_mat, index=['0','1'], columns=['0', '1'],)\nfig = plt.figure(figsize=[10,7])\nheatmap = sns.heatmap(df_cm, annot=True, fmt=\"d\")\nheatmap.yaxis.set_ticklabels(heatmap.yaxis.get_ticklabels(), rotation=0, ha='right', fontsize=14)\nheatmap.xaxis.set_ticklabels(heatmap.xaxis.get_ticklabels(), rotation=45, ha='right', fontsize=14)\nplt.ylabel('True label')\nplt.xlabel('Predicted label')", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "print(classification_report(y_test, logreg.predict(X_test)))", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "print(classification_report(y_train, logreg.predict(X_train)))", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "For logistic regression, training and test performance are very similar. This probably means that the created model generalizes well on new data. "}, {"metadata": {}, "cell_type": "markdown", "source": "### Building many models"}, {"metadata": {}, "cell_type": "markdown", "source": "When building and comparing lots of models, it may be useful to loop over several classifiers or over one classifier with several parameters. An idea to overcome the overfitting problem with tree-based classifiers is to limit the depth of trees and inspect evaluation metrics."}, {"metadata": {}, "cell_type": "code", "source": "# vary maximum tree depth for random forest\ntree_depth = [5, 10, 20]\nfor i in tree_depth:\n    rf = RandomForestClassifier(max_depth=i)\n    rf.fit(X_train, y_train)\n    print('Max tree depth: ', i)\n    print('Train results: ', classification_report(y_train, rf.predict(X_train)))\n    print('Test results: ',classification_report(y_test, rf.predict(X_test)))", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "Feel free to consider additional aspects if you are familiar with machine learning: You could check for class imbalance and mitigate this by oversampling training data. You could also try more classification algorithms like SVM. "}, {"metadata": {}, "cell_type": "markdown", "source": "# Titanic Deployment"}, {"metadata": {}, "cell_type": "markdown", "source": "In this section you will learn how to deploy a scikit-learn model as a web service with the aid of the _IBM Watson Machine Learning Service_."}, {"metadata": {}, "cell_type": "markdown", "source": "## Local Prediction"}, {"metadata": {}, "cell_type": "markdown", "source": "First, choose the model to deploy and make a local prediction. Later, you will compare the predicted results returned by the local model with the results returned by the web service for evaluation purposes. "}, {"metadata": {}, "cell_type": "code", "source": "# assign your favorite model to the deployment_classifier variable\ndeployment_classifier = logreg\ndeployment_classifier", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "# recap: print first rows of training data\ndf_dummies.head(2)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "# recap: print first rows of predictors (here: training data without predicted column \"Survived\")\npredictors.head(2)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "# use the local model to make a prediction for the first two passengers\ndeployment_classifier.predict(predictors.iloc[0:2])", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "Review the output. Did the model return a correct prediction for the \"Survived\" field?"}, {"metadata": {}, "cell_type": "markdown", "source": "## Web service deployment"}, {"metadata": {}, "cell_type": "markdown", "source": "To work through this section, you need\n- a Watson Machine Learning instance\n- an IBM Cloud API key\n- a deployment space id\n\nCheck the guidelines available at https://github.com/ellenhvn/hhz-artificial-intelligence-vl-ws22/tree/main/Guidelines for details."}, {"metadata": {}, "cell_type": "code", "source": "# import Python client library (documentation available at http://ibm-wml-api-pyclient.mybluemix.net/)\nfrom ibm_watson_machine_learning import APIClient", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "# set your IBM Cloud API key \napi_key = \"...\"\n\n# set the URL of your WML instance \n# depending on the region you chose during instance creation it will take one of the below values:\n# - Frankfurt: https://eu-de.ml.cloud.ibm.com\n# - Dallas: https://us-south.ml.cloud.ibm.com\n# - London: https://eu-gb.ml.cloud.ibm.com\n# - Tokyo: https://jp-tok.ml.cloud.ibm.com\nwml_url = \"https://us-south.ml.cloud.ibm.com\"", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "# setup the API client\nwml_client = APIClient({\n   \"url\": wml_url,\n   \"apikey\": api_key\n})", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "# list all existing deployment spaces\nwml_client.spaces.list()", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "# set the id of the deployment space you want to use as default\nwml_client.set.default_space(\"...\")", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "# setup required properties to store the model\nsofware_spec_uid = wml_client.software_specifications.get_id_by_name(\"runtime-22.2-py3.10\")\nmetadata = {\n            wml_client.repository.ModelMetaNames.NAME: 'Titanic Model',\n            wml_client.repository.ModelMetaNames.TYPE: 'scikit-learn_1.1',\n            wml_client.repository.ModelMetaNames.SOFTWARE_SPEC_UID: sofware_spec_uid\n}\nmetadata", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "# store the scikit-learn model in WML\nmodel = wml_client.repository.store_model(deployment_classifier, meta_props=metadata)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "# review available models in your WML instance\nwml_client.repository.list()", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "# retrieve the id of the model you deployed\npublished_model_uid = wml_client.repository.get_model_id(model)\npublished_model_uid", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "# setup required properties to deploy the model\nmetadata = {\n    wml_client.deployments.ConfigurationMetaNames.NAME: \"Deployment of Titanic model\",\n    wml_client.deployments.ConfigurationMetaNames.ONLINE: {}\n}\n", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "# deploy the model as a web service (an API endpoint is generated for your deployment so your tools and apps can use a REST API to send data to your deployed model for analysis)\ncreated_deployment = wml_client.deployments.create(published_model_uid, name=\"Titanic Deployment\", meta_props=metadata)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "# keep the REST API endpoint for evaluation\nscoring_endpoint = wml_client.deployments.get_scoring_href(created_deployment)\nscoring_endpoint", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "Now check the deployment spaces UI (open the menu on the side menu, select \"View all spaces\" and navigate to the selected space).\n- Can you find your model and deployment?\n- Can you find code snippets to share with developers that will enable them to make predictions using your web service deployment?"}, {"metadata": {}, "cell_type": "markdown", "source": "## Evaluate web service deployment"}, {"metadata": {}, "cell_type": "markdown", "source": "You will now use the REST API (documentation available at https://cloud.ibm.com/apidocs/machine-learning#deployments-compute-predictions) to execute a prediction and compare its results against the local prediction from a previous section."}, {"metadata": {}, "cell_type": "code", "source": "# import requests module\nimport requests", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "# setup the request payload as per the API documentation\nscoring_values = predictors.iloc[0:2].to_numpy().tolist()\npayload_scoring = {\"input_data\": [{\"values\": scoring_values}]}", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "# create a token to make an authenticated request\ntoken_response = requests.post('https://iam.eu-de.bluemix.net/identity/token', data={\"apikey\": api_key, \"grant_type\": 'urn:ibm:params:oauth:grant-type:apikey'})\nmltoken = token_response.json()[\"access_token\"]\n", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "# send the scoring request\nheader = {'Content-Type': 'application/json', 'Authorization': 'Bearer ' + mltoken}\nresponse_scoring = requests.post(f'{scoring_endpoint}?version=2022-04-29', json=payload_scoring, headers={'Authorization': 'Bearer ' + mltoken})\nresponse_scoring.content", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "- Do the results match the predictions executed in this notebook?\n- What information does the response payload include in addition to the classification?"}, {"metadata": {}, "cell_type": "markdown", "source": "## Clean up"}, {"metadata": {}, "cell_type": "markdown", "source": "Free WML instances only allow for a limited number of models and deployments. Let's clean up artefacts that are no longer needed."}, {"metadata": {}, "cell_type": "code", "source": "# list deployments\nwml_client.deployments.list()", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "# delete deployments you just created \nwml_client.deployments.delete(\"...\")", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "# list models\nwml_client.repository.list_models()", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "# delete models you just created\nwml_client.repository.delete(\"...\")", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "", "execution_count": null, "outputs": []}], "metadata": {"kernelspec": {"name": "python3", "display_name": "Python 3.10", "language": "python"}, "language_info": {"name": "python", "version": "3.10.9", "mimetype": "text/x-python", "codemirror_mode": {"name": "ipython", "version": 3}, "pygments_lexer": "ipython3", "nbconvert_exporter": "python", "file_extension": ".py"}}, "nbformat": 4, "nbformat_minor": 2}