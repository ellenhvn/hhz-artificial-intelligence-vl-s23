{"cells": [{"metadata": {}, "cell_type": "markdown", "source": "# Titanic Data Preparation"}, {"metadata": {}, "cell_type": "markdown", "source": "## CRISP-DM"}, {"metadata": {}, "cell_type": "markdown", "source": "CRISP-DM is the \"CRoss Industry Standard Process for Data Mining\" which is one of the frequently used guidelines in data analytics. \n\nThis notebook regards the phases \"Data Understanding\" and \"Data Preparation\". "}, {"metadata": {}, "cell_type": "code", "source": "from IPython import display\ndisplay.Image('https://upload.wikimedia.org/wikipedia/commons/b/b9/CRISP-DM_Process_Diagram.png', width=500)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "## Import relevant packages"}, {"metadata": {}, "cell_type": "code", "source": "import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set()\n%matplotlib inline", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "## Load and inspect data set"}, {"metadata": {}, "cell_type": "markdown", "source": "The Kaggle Titanic data set is often used to do first steps in data science / machine learning. It contains certain personal information about passengers. The task is to prepare raw data and create prediction models, so-called classifiers, that predict if a person survived or not. Data are available here: https://www.kaggle.com/c/titanic/data - refer to this page also for further information, e.g. a data dictionary."}, {"metadata": {}, "cell_type": "markdown", "source": "Attention: Reading and writing files is slightly different between Local and Cloud. \nYou can click on the 0100 icon in the upper right corner, locate the file and then click \"Insert to code\". \nMake sure that the dataframe variable name is _original_data_ "}, {"metadata": {}, "cell_type": "code", "source": "# Local: Fetch the file\n#original_data = pd.read_csv('train.csv') # Local, use full path if notebook and file in different folders! \n\n# Cloud: Fetch the file\n# here: insert to code\n\noriginal_data = pd.read_csv(body)\noriginal_data.head()\n", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "original_data.head(5)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "original_data.describe(include='all') # descriptive statistics for all columns", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "## Check for duplicates"}, {"metadata": {}, "cell_type": "code", "source": "original_data[original_data.duplicated(keep=False)] # check for duplicate rows", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "original_data['PassengerId'][original_data['PassengerId'].duplicated(keep=False)] # check for duplicate PassengerIds ", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "There are no obvious duplicates in the data set. "}, {"metadata": {}, "cell_type": "markdown", "source": "## Check for null values and adjust data sets"}, {"metadata": {}, "cell_type": "code", "source": "original_data.isnull().sum()", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "There are three columns that contain missing values. Simple ways to handle this data quality issues are \n\na) drop the rows where a specific value is missing (here done for 'Age'), \n\nb) drop the whole column if too many values are missing (here: 'Cabin'), \n\nc) replace missing values with the most frequent value (here: 'Embarked'). "}, {"metadata": {}, "cell_type": "code", "source": "df_wo_null = original_data.dropna(axis=0, subset=['Age']) # drop rows where 'Age' is missing", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "df_wo_null.isnull().sum()", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "df_wo_null = df_wo_null.drop(['Cabin'], axis = 1) # drop column 'Cabin' since there are too many missing values", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "df_wo_null.head()", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "df_wo_null['Embarked'] = df_wo_null['Embarked'].fillna(df_wo_null['Embarked'].mode().iloc[0]) # replace missing 'Embarked' with the most frequent value", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "## Find predictors and edit data"}, {"metadata": {}, "cell_type": "markdown", "source": "In the next step, predictors are defined. These are columns (from now on called features) that are assumed to have an influence on the target, in this case 'Survived: yes or no?'. Besides, new features are derived from existing ones. This process is called 'feature engineering' and is one of the key steps during data preparation. "}, {"metadata": {}, "cell_type": "markdown", "source": "### Drop features"}, {"metadata": {}, "cell_type": "code", "source": "df_wo_null = df_wo_null.drop(['Ticket','PassengerId'], axis = 1) # IDs are no suitable predictors\ndf_wo_null = df_wo_null.drop(['Name'], axis = 1) # 'Name' is no appropriate predictor", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "### Survival rate per feature value"}, {"metadata": {}, "cell_type": "code", "source": "df_wo_null[['Pclass', 'Survived']].groupby(['Pclass'], as_index=False).mean().sort_values(by='Survived', ascending=False)\n# survival rate per class ", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "df_wo_null[[\"Sex\", \"Survived\"]].groupby(['Sex'], as_index=False).mean().sort_values(by='Survived', ascending=False)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "viz = sns.FacetGrid(df_wo_null, col='Survived')\nviz.map(plt.hist, 'Age', bins=40)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "### Creating new features"}, {"metadata": {}, "cell_type": "code", "source": "df_wo_null['AgeRange'] = pd.cut(df_wo_null['Age'], bins=8) # summarize 'Age' in ranges\ndf_wo_null[['AgeRange', 'Survived']].groupby(['AgeRange'], as_index=False).mean().sort_values(by='AgeRange', ascending=True)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "df_wo_null['AgeRange'] = pd.cut(df_wo_null['Age'], bins=8, labels = ['0-10','10-20','20-30','30-40','40-50','50-60','60-70', '70-80'])\n# create a new column 'AgeRange'", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "df_wo_null = df_wo_null.drop(['Age'], axis=1) # remove 'Age' since information is now contained in 'AgeRange'", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "df_wo_null.head()", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "viz = sns.FacetGrid(df_wo_null, col='Survived')\nviz.map(plt.hist, 'Fare', bins=4)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "df_wo_null['FareRange'] = pd.qcut(df_wo_null['Fare'], q=4) # create quartiles from fare price \ndf_wo_null[['FareRange', 'Survived']].groupby(['FareRange'], as_index=False).mean().sort_values(by='FareRange', ascending=True)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "df_wo_null['FareRange'] = pd.qcut(df_wo_null['Fare'], q=4, labels = ['Q1', 'Q2', 'Q3', 'Q4']) # add 'FareRange' as a new column\ndf_wo_null.head()", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "df_wo_null = df_wo_null.drop(['Fare'], axis = 1) # remove 'Fare' since information is now contained in 'FareRange' ", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "df_wo_null['Family'] = df_wo_null['SibSp'] + df_wo_null['Parch'] + 1 # calculate family size and add this as a new column\n\ndf_wo_null[['Family', 'Survived']].groupby(['Family'], as_index=False).mean().sort_values(by='Survived', ascending=False)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "df_wo_null = df_wo_null.drop(['SibSp', 'Parch'], axis = 1) # remove columns since information is now contained in 'Family'", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "df_wo_null.head()", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "df_wo_null.isnull().sum()", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "Feel free to try out additional idea, e.g. extract title from name. "}, {"metadata": {}, "cell_type": "code", "source": "df_wo_null.head()", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "## Create data for modeling"}, {"metadata": {"scrolled": true}, "cell_type": "code", "source": "df_dummies = pd.get_dummies(df_wo_null, drop_first=True) # 0-1 encoding for categorical values\ndf_dummies.head()", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "df_dummies.columns", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "Attention: Select the correct way to export csv file here. \n\nIf you want to write a file to the project, you can follow these instructions to use ws-lib: https://dataplatform.cloud.ibm.com/docs/content/wsj/analyze-data/ws-lib-python.html?audience=wdp"}, {"metadata": {}, "cell_type": "code", "source": "# Local\n# df_dummies.to_csv('train_dummies.csv', index = False) # full path if file should not be in the same folder as the notebook\n\n# Cloud\n#wslib ", "execution_count": 2, "outputs": []}], "metadata": {"kernelspec": {"name": "python3", "display_name": "Python 3.9", "language": "python"}, "language_info": {"name": "python", "version": "3.9.13", "mimetype": "text/x-python", "codemirror_mode": {"name": "ipython", "version": 3}, "pygments_lexer": "ipython3", "nbconvert_exporter": "python", "file_extension": ".py"}}, "nbformat": 4, "nbformat_minor": 4}